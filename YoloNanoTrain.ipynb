{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YoloNano论文复现\n",
    "\n",
    "《YOLO Nano: a Highly Compact You Only Look Once Convolutional Neural Network for Object Detection》"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 网络模块定义\n",
    "## 1.1 EP(Expansion-projection) module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EP(nn.Module):\n",
    "    def __init__(self,input_channel,output_channel,expansion=0.5,stride=2):\n",
    "        super(EP,self).__init__()\n",
    "        # conv2D(input_channel,output_channal,k_size)\n",
    "        self.dw_channel = int(expansion*input_channel)\n",
    "        self.input_channel = input_channel\n",
    "        self.output_channel = output_channel\n",
    "        \n",
    "        self.conv1_1_1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channel,self.dw_channel,1),\n",
    "            nn.BatchNorm2d(self.dw_channel),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.dwconv = nn.Sequential(\n",
    "            nn.Conv2d(self.dw_channel,self.dw_channel,3,groups=self.dw_channel,stride=stride,padding=1),\n",
    "            nn.BatchNorm2d(self.dw_channel),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.conv1_1_2 = nn.Sequential(\n",
    "            nn.Conv2d(self.dw_channel,self.output_channel,1),\n",
    "            nn.BatchNorm2d(self.output_channel),\n",
    "        )\n",
    "    def forward(self,Input):\n",
    "        Output = self.conv1_1_1(Input)\n",
    "        Output = self.dwconv(Output)\n",
    "        Output = self.conv1_1_2(Output)\n",
    "        if self.input_channel == self.output_channel:\n",
    "            Output+=Input\n",
    "        return Output\n",
    "    \n",
    "# test = torch.ones((1,24,208,208))\n",
    "# ep = EP(24,70)\n",
    "# print(ep(test).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 PEP module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PEP(nn.Module):\n",
    "    def __init__(self,input_channel,projection_channel,output_channel=None,expansion=0.5):\n",
    "        super(PEP,self).__init__()\n",
    "        # conv2D(input_channel,output_channal,k_size)\n",
    "        self.input_channel = input_channel\n",
    "        self.projection_channel = projection_channel\n",
    "        self.output_channel = output_channel\n",
    "        expand_channel = int(expansion*input_channel)\n",
    "        self.conv1_1_1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channel,self.projection_channel,1),\n",
    "            nn.BatchNorm2d(self.projection_channel),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.conv1_1_2 = nn.Sequential(\n",
    "            nn.Conv2d(self.projection_channel,expand_channel,1),\n",
    "            nn.BatchNorm2d(expand_channel),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.dwconv = nn.Sequential(\n",
    "            nn.Conv2d(expand_channel,expand_channel,3,groups=expand_channel,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(expand_channel),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        if self.output_channel == None:\n",
    "            self.conv1_1_3 = nn.Sequential(\n",
    "                nn.Conv2d(expand_channel,self.input_channel,1),\n",
    "                nn.BatchNorm2d(self.input_channel),\n",
    "            )\n",
    "        else:\n",
    "            self.conv1_1_3 = nn.Sequential(\n",
    "                nn.Conv2d(expand_channel,self.output_channel,1),\n",
    "                nn.BatchNorm2d(self.output_channel),\n",
    "            )\n",
    "    def forward(self,Input):\n",
    "        Output = self.conv1_1_1(Input)\n",
    "        Output = self.conv1_1_2(Output)\n",
    "        Output = self.dwconv(Output)\n",
    "        Output = self.conv1_1_3(Output)\n",
    "        if self.output_channel == None:\n",
    "            Output+=Input\n",
    "#         print(Input.shape,Output.shape)\n",
    "        return Output\n",
    "    \n",
    "# test = torch.ones((1,24,208,208))\n",
    "# pep = PEP(24,70)\n",
    "# print(pep(test).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 FCA module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCA(nn.Module):\n",
    "    def __init__(self, channels, reduction_ratio):\n",
    "        super(FCA, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "\n",
    "        hidden_channels = channels // reduction_ratio\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, hidden_channels, bias=False),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Linear(hidden_channels, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        out = self.avg_pool(x).view(b, c)\n",
    "        out = self.fc(out).view(b, c, 1, 1)\n",
    "        out = x * out.expand_as(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Yolo Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_wh_iou(wh1, wh2):\n",
    "    wh2 = wh2.t()\n",
    "    w1, h1 = wh1[0], wh1[1]\n",
    "    w2, h2 = wh2[0], wh2[1]\n",
    "    inter_area = torch.min(w1, w2) * torch.min(h1, h2)\n",
    "    union_area = (w1 * h1 + 1e-16) + w2 * h2 - inter_area\n",
    "    return inter_area / union_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_iou(box1, box2, x1y1x2y2=True):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes\n",
    "    \"\"\"\n",
    "    if not x1y1x2y2:\n",
    "        # Transform from center and width to exact coordinates\n",
    "        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
    "        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
    "        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
    "        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
    "    else:\n",
    "        # Get the coordinates of bounding boxes\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
    "\n",
    "    # get the corrdinates of the intersection rectangle\n",
    "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n",
    "    # Intersection area\n",
    "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(\n",
    "        inter_rect_y2 - inter_rect_y1 + 1, min=0\n",
    "    )\n",
    "    # Union Area\n",
    "    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
    "    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
    "\n",
    "    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_targets(pred_boxes, pred_cls, target, anchors, ignore_thres):\n",
    "\n",
    "    #print(pred_boxes.size(), pred_cls.size(), target.size(), anchors.size())\n",
    "\n",
    "    ByteTensor = torch.cuda.ByteTensor if pred_boxes.is_cuda else torch.ByteTensor\n",
    "    FloatTensor = torch.cuda.FloatTensor if pred_boxes.is_cuda else torch.FloatTensor\n",
    "\n",
    "    nB = pred_boxes.size(0) # batch_num\n",
    "    nA = pred_boxes.size(1) # anchor_num = 3\n",
    "    nC = pred_cls.size(-1) # classes_num = 20\n",
    "    nG = pred_boxes.size(2) # grid_num = 52\n",
    "\n",
    "    # Output tensors\n",
    "    obj_mask = ByteTensor(nB, nA, nG, nG).fill_(0)\n",
    "    noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(1)\n",
    "    class_mask = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tx = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tw = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    th = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0)\n",
    "\n",
    "#     Convert to position relative to box\n",
    "    target_boxes = target[:, 2:6] * nG\n",
    "#     print(target_boxes.shape,target.shape)\n",
    "    gxy = target_boxes[:, :2]\n",
    "    gwh = target_boxes[:, 2:]\n",
    "    # Get anchors with best iou\n",
    "#     print(gwh.shape)\n",
    "    ious = torch.stack([bbox_wh_iou(anchor, gwh) for anchor in anchors])\n",
    "    #print(ious.size(), gwh.size(), target.size())\n",
    "\n",
    "    best_ious, best_n = ious.max(0)\n",
    "#     print(best_ious.size(), best_n.size())\n",
    "\n",
    "#     Separate target values\n",
    "    b, target_labels = target[:, :2].long().t()\n",
    "    gx, gy = gxy.t()\n",
    "    gw, gh = gwh.t()\n",
    "    gi, gj = gxy.long().t()\n",
    "    # Set masks\n",
    "#     print(b,best_n,gj,gi,obj_mask.shape)\n",
    "    obj_mask[b, best_n, gj, gi] = 1\n",
    "    noobj_mask[b, best_n, gj, gi] = 0\n",
    "\n",
    "    # Set noobj mask to zero where iou exceeds ignore threshold\n",
    "    for i, anchor_ious in enumerate(ious.t()):\n",
    "        #print(noobj_mask.size())\n",
    "        #print(b[i], anchor_ious>ignore_thres, gj[i], gi[i])\n",
    "        noobj_mask[b[i], anchor_ious > ignore_thres, gj[i], gi[i]] = 0\n",
    "\n",
    "    # Coordinates\n",
    "    tx[b, best_n, gj, gi] = gx - gx.floor()\n",
    "    ty[b, best_n, gj, gi] = gy - gy.floor()\n",
    "    # Width and height\n",
    "    tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, 0] + 1e-16)\n",
    "    th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, 1] + 1e-16)\n",
    "\n",
    "    #print(tw[b,best_n, gj, gi])\n",
    "\n",
    "    # One-hot encoding of label\n",
    "    tcls[b, best_n, gj, gi, target_labels] = 1\n",
    "    # Compute label correctness and iou at best anchor\n",
    "    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(-1) == target_labels).float()\n",
    "    iou_scores[b, best_n, gj, gi] = bbox_iou(pred_boxes[b, best_n, gj, gi], target_boxes, x1y1x2y2=False)\n",
    "\n",
    "    tconf = obj_mask.float()\n",
    "    return iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOLayer(nn.Module):\n",
    "    # detection layer\n",
    "    def __init__(self, anchors, num_classes, img_dim=416):\n",
    "        super(YOLOLayer, self).__init__()\n",
    "        self.anchors = anchors\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_thres = .5\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        self.obj_scale = 1\n",
    "        self.noobj_scale = 100\n",
    "        self.metrics = {}\n",
    "        self.img_dim = img_dim\n",
    "        self.grid_size = 0\n",
    "\n",
    "    def compute_grid_offsets(self, grid_size, cuda=True):\n",
    "        self.grid_size = grid_size\n",
    "        g = self.grid_size\n",
    "        FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "        self.stride = self.img_dim / self.grid_size\n",
    "        # Calculate offsets for each grid\n",
    "        self.grid_x = torch.arange(g).repeat(g, 1).view([1, 1, g, g]).type(FloatTensor)\n",
    "        self.grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, g, g]).type(FloatTensor)\n",
    "        self.scaled_anchors = FloatTensor([(a_w / self.stride, a_h / self.stride) for a_w, a_h in self.anchors])\n",
    "        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n",
    "        self.anchor_h = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n",
    "\n",
    "    def forward(self, x, targets=None, img_dim=416):\n",
    "        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n",
    "        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n",
    "        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\n",
    "\n",
    "        self.img_dim = img_dim\n",
    "\n",
    "        num_samples = x.size(0)\n",
    "        grid_size = x.size(2)\n",
    "\n",
    "        prediction = (\n",
    "            x.view(num_samples, self.num_anchors, self.num_classes+5, grid_size, grid_size)\n",
    "            .permute(0,1,3,4,2)\n",
    "            .contiguous()\n",
    "        )\n",
    "#         print(prediction.shape)\n",
    "#         torch.Size([1, 3, 26, 26, 25])\n",
    "        x = torch.sigmoid(prediction[..., 0]) # center x\n",
    "        y = torch.sigmoid(prediction[..., 1]) # center y\n",
    "        w = prediction[..., 2] # width\n",
    "        h = prediction[..., 3] # Height\n",
    "        pred_conf = torch.sigmoid(prediction[..., 4]) # Conf # bbox的置信度\n",
    "        pred_cls = torch.sigmoid(prediction[..., 5:]) # Cls Pred\n",
    "\n",
    "        if grid_size != self.grid_size:\n",
    "            self.compute_grid_offsets(grid_size, cuda=x.is_cuda)\n",
    "\n",
    "        # Add offset and scale with anchors\n",
    "        pred_boxes = FloatTensor(prediction[..., :4].shape)\n",
    "        pred_boxes[..., 0] = x.data + self.grid_x\n",
    "        pred_boxes[..., 1] = y.data + self.grid_y\n",
    "        pred_boxes[..., 2] = torch.exp(w.data) * self.anchor_w\n",
    "        pred_boxes[..., 3] = torch.exp(h.data) * self.anchor_h\n",
    "#         print(pred_boxes.size())\n",
    "#         torch.Size([1, 3, 52, 52, 4])\n",
    "#         print(pred_boxes.view(num_samples, -1, 4).size())\n",
    "\n",
    "        output = torch.cat(\n",
    "            (\n",
    "                pred_boxes.view(num_samples, -1, 4) * self.stride,# 真实的bbox的值\n",
    "                pred_conf.view(num_samples, -1, 1), # bbox置信度\n",
    "                pred_cls.view(num_samples, -1, self.num_classes), # 每一类的一个置信度\n",
    "            ),\n",
    "            -1,\n",
    "        )\n",
    "\n",
    "        if targets is None:\n",
    "            return output, 0\n",
    "        else:\n",
    "            iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf = build_targets(\n",
    "                pred_boxes=pred_boxes,\n",
    "                pred_cls=pred_cls,\n",
    "                target=targets,\n",
    "                anchors=self.scaled_anchors,\n",
    "                ignore_thres=self.ignore_thres,\n",
    "            )\n",
    "\n",
    "            # Loss : Mask outputs to ignore non-existing objects (except with conf. loss)\n",
    "#             print('x:',x[obj_mask],tx[obj_mask])\n",
    "#             print('y:',y[obj_mask],ty[obj_mask])\n",
    "#             print('w:',w[obj_mask],tw[obj_mask])\n",
    "#             print('h:',h[obj_mask],th[obj_mask])\n",
    "            loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])\n",
    "            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])\n",
    "            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])\n",
    "            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])\n",
    "            #print(pred_conf[obj_mask], tconf[obj_mask])\n",
    "            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])\n",
    "            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])\n",
    "            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj\n",
    "            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])\n",
    "            total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n",
    "\n",
    "            # Metrics\n",
    "            cls_acc = 100 * class_mask[obj_mask].mean()\n",
    "            conf_obj = pred_conf[obj_mask].mean()\n",
    "            conf_noobj = pred_conf[noobj_mask].mean()\n",
    "            conf50 = (pred_conf > 0.5).float()\n",
    "            iou50 = (iou_scores > 0.5).float()\n",
    "            iou75 = (iou_scores > 0.75).float()\n",
    "            detected_mask = conf50 * class_mask * tconf\n",
    "            precision = torch.sum(iou50 * detected_mask) / (conf50.sum() + 1e-16)\n",
    "            recall50 = torch.sum(iou50 * detected_mask) / (obj_mask.sum() + 1e-16)\n",
    "            recall75 = torch.sum(iou75 * detected_mask) / (obj_mask.sum() + 1e-16)\n",
    "#             self.metrics = {\n",
    "#                 \"loss\": to_cpu(total_loss).item(),\n",
    "#                 \"x\": to_cpu(loss_x).item(),\n",
    "#                 \"y\": to_cpu(loss_y).item(),\n",
    "#                 \"w\": to_cpu(loss_w).item(),\n",
    "#                 \"h\": to_cpu(loss_h).item(),\n",
    "#                 \"conf\": to_cpu(loss_conf).item(),\n",
    "#                 \"cls\": to_cpu(loss_cls).item(),\n",
    "#                 \"cls_acc\": to_cpu(cls_acc).item(),\n",
    "#                 \"recall50\": to_cpu(recall50).item(),\n",
    "#                 \"recall75\": to_cpu(recall75).item(),\n",
    "#                 \"precision\": to_cpu(precision).item(),\n",
    "#                 \"conf_obj\": to_cpu(conf_obj).item(),\n",
    "#                 \"conf_noobj\": to_cpu(conf_noobj).item(),\n",
    "#                 \"grid_size\": grid_size,\n",
    "#             }\n",
    "            return output, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 整体网络代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class YoloNano(nn.Module):\n",
    "    def __init__(self, anchor_num=3, class_num = 20):\n",
    "        super(YoloNano,self).__init__()\n",
    "        self.anchor_num = anchor_num\n",
    "        self.class_num = class_num\n",
    "        self.c1 = nn.Sequential(\n",
    "            nn.Conv2d(3,12,3,padding=1),#(12,416,416)\n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.c2 = nn.Sequential(\n",
    "            nn.Conv2d(12,24,3,stride=2,padding=1),#(24,208,208)\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.pep1 = PEP(24,7)#(24,208,208)\n",
    "        self.ep1 = EP(24,70)#(70,104,104)\n",
    "        self.pep2 = PEP(70,25)#(70,104,104)\n",
    "        self.pep3 = PEP(70,24)#(70,104,104)\n",
    "        self.ep2 = EP(70,150)#(150,52,52)\n",
    "        self.pep4 = PEP(150,56)#(150,52,52)\n",
    "        self.c3 = nn.Sequential(\n",
    "            nn.Conv2d(150,150,1,stride=1),#(150,52,52)\n",
    "            nn.BatchNorm2d(150),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fca = FCA(150,8)\n",
    "        self.pep5 = PEP(150,73)# (150,52,52)\n",
    "        self.pep6 = PEP(150,71)# (150,52,52)\n",
    "        self.pep7 = PEP(150,75)# (150,52,52)\n",
    "        self.ep3 = EP(150,325) # (325,26,26)\n",
    "        self.pep8 = PEP(325,132)# (325,26,26)\n",
    "        self.pep9 = PEP(325,124)# (325,26,26)\n",
    "        self.pep10 = PEP(325,141)# (325,26,26)\n",
    "        self.pep11= PEP(325,140)# (325,26,26)\n",
    "        self.pep12 = PEP(325,137)# (325,26,26)\n",
    "        self.pep13 = PEP(325,135)# (325,26,26)\n",
    "        self.pep14 = PEP(325,133)# (325,26,26)\n",
    "        self.pep15 = PEP(325,140)# (325,26,26)\n",
    "        self.ep4 = EP(325,545) # (545,13,13)\n",
    "        self.pep16 = PEP(545,276)# (545,13,13)\n",
    "        self.c4 = nn.Sequential(\n",
    "            nn.Conv2d(545,230,1),#(230,13,13)\n",
    "            nn.BatchNorm2d(230),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.ep5 = EP(230,489,stride=1) # (489,13,13)\n",
    "        self.pep17 = PEP(489,213,output_channel=469)# (469,13,13)\n",
    "        self.c5 = nn.Sequential(\n",
    "            nn.Conv2d(469,189,1),#(189,13,13)\n",
    "            nn.BatchNorm2d(189),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.c6 = nn.Sequential(\n",
    "            nn.Conv2d(189,105,1),#(189,13,13)\n",
    "            nn.BatchNorm2d(105),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.upsample1 = nn.UpsamplingBilinear2d(size=(26,26))\n",
    "        self.pep18 = PEP(430,113,output_channel=325)# (325,26,26)\n",
    "        self.pep19 = PEP(325,113,output_channel=207)# (207,26,26)\n",
    "        self.c7 = nn.Sequential(\n",
    "            nn.Conv2d(207,98,1),#(98,26,26)\n",
    "            nn.BatchNorm2d(98),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.c8 = nn.Sequential(\n",
    "            nn.Conv2d(98,47,1),#(47,26,26)\n",
    "            nn.BatchNorm2d(47),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.upsample2 = nn.UpsamplingBilinear2d(size=(52,52))\n",
    "        self.pep20 = PEP(197,58,output_channel=122)# (207,52,52)\n",
    "        self.pep21 = PEP(122,52,output_channel=87)# (87,52,52)\n",
    "        self.pep22 = PEP(87,47,output_channel=93)# (93,52,52)\n",
    "        self.c9 = nn.Conv2d(93,self.anchor_num*(5+self.class_num),1)#(75,52,52)\n",
    "        self.ep6 = EP(98,183,stride=1) # (183,26,26)\n",
    "        self.c10 = nn.Conv2d(183,self.anchor_num*(5+self.class_num),1)#(75,26,26)\n",
    "        self.ep7 = EP(189,462,stride=1) # (462,13,13)\n",
    "        self.c11 = nn.Conv2d(462,self.anchor_num*(5+self.class_num),1)#(75,13,13)\n",
    "        \n",
    "        anchors52 = [[61,9], [17,22], [22,50]] # 52x52\n",
    "        anchors26 = [[36, 30], [43, 65], [68, 41]] # 26x26\n",
    "        anchors13 = [[156, 134], [67, 107], [108, 63]] # 13x13\n",
    "        self.yolo52=YOLOLayer(anchors52,class_num)\n",
    "        self.yolo26=YOLOLayer(anchors26,class_num)\n",
    "        self.yolo13=YOLOLayer(anchors13,class_num)\n",
    "        \n",
    "        \n",
    "    def forward(self, Input, targets = None):\n",
    "        Output1 = self.c1(Input)\n",
    "        Output1 = self.c2(Output1)\n",
    "        Output1 = self.pep1(Output1)\n",
    "        Output1 = self.ep1(Output1)\n",
    "        Output1 = self.pep2(Output1)\n",
    "        Output1 = self.pep3(Output1)\n",
    "        Output1 = self.ep2(Output1)\n",
    "        Output1 = self.pep4(Output1)\n",
    "        Output1 = self.c3(Output1)\n",
    "        Output1 = self.fca(Output1)\n",
    "        Output1 = self.pep5(Output1)\n",
    "        Output1 = self.pep6(Output1)\n",
    "        Output1 = self.pep7(Output1)\n",
    "        \n",
    "        Output2 = self.ep3(Output1)\n",
    "        Output2 = self.pep8(Output2)\n",
    "        Output2 = self.pep9(Output2)\n",
    "        Output2 = self.pep10(Output2)\n",
    "        Output2 = self.pep11(Output2)\n",
    "        Output2 = self.pep12(Output2)\n",
    "        Output2 = self.pep13(Output2)\n",
    "        Output2 = self.pep14(Output2)\n",
    "        Output2 = self.pep15(Output2)\n",
    "        \n",
    "        Output3 = self.ep4(Output2)\n",
    "        Output3 = self.pep16(Output3)\n",
    "        Output3 = self.c4(Output3)\n",
    "        Output3 = self.ep5(Output3)\n",
    "        Output3 = self.pep17(Output3)\n",
    "        Output3 = self.c5(Output3)\n",
    "        \n",
    "        Output4 = self.c6(Output3)\n",
    "        Output4 = self.upsample1(Output4)\n",
    "        Output4 = torch.cat((Output4, Output2),dim=1)\n",
    "        Output4 = self.pep18(Output4)\n",
    "        Output4 = self.pep19(Output4)\n",
    "        Output4 = self.c7(Output4)\n",
    "        \n",
    "        Output5 = self.c8(Output4)\n",
    "        Output5 = self.upsample2(Output5)\n",
    "        Output5 = torch.cat((Output5, Output1),dim=1)\n",
    "        Output5 = self.pep20(Output5)\n",
    "        Output5 = self.pep21(Output5)\n",
    "        Output5 = self.pep22(Output5)\n",
    "        Output5 = self.c9(Output5)#输出75 52 52 # (52,52,anchor_num*(5+class_num))\n",
    "        Output5,loss1 = self.yolo52(x=Output5,targets = targets)\n",
    "        \n",
    "        Output4 = self.ep6(Output4)\n",
    "        Output4 = self.c10(Output4)#输出75 26 26# (26,26,anchor_num*(5+class_num))\n",
    "        Output4,loss2 = self.yolo26(x=Output4,targets =targets)\n",
    "        \n",
    "        Output3 = self.ep7(Output3)\n",
    "        Output3 = self.c11(Output3)#输出75 13 13# (13,13,anchor_num*(5+class_num))\n",
    "        Output3,loss3 = self.yolo13(x=Output3,targets =targets)\n",
    "        loss= loss1+loss2+loss3\n",
    "        \n",
    "        return Output3,Output4,Output5,loss\n",
    "    \n",
    "# test = torch.zeros((6,6))\n",
    "# data = torch.ones((1,3,416,416))\n",
    "# nano = YoloNano(class_num=2)\n",
    "# start = time.time()\n",
    "# output1,output2,output3,loss = nano(Input = data,targets = test)\n",
    "# print((time.time()-start)/10)\n",
    "# print(output1.shape,output2.shape,output3.shape,loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. VOC数据导入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 导入单个数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "class Bbox(object):\n",
    "    '''\n",
    "    Bbox是标注框的相关信息：\n",
    "    Variable name:\n",
    "        type: string, eg:'robot' or 'deck'\n",
    "        xmin: int\n",
    "        ymin: int\n",
    "        xmax: int\n",
    "        ymax: int\n",
    "    '''\n",
    "    def __init__(self,type,xmin,ymin,xmax,ymax):\n",
    "        self.type = type\n",
    "        self.xmax = int(xmax)\n",
    "        self.xmin = int(xmin)\n",
    "        self.ymax = int(ymax)\n",
    "        self.ymin = int(ymin)\n",
    "\n",
    "class Data(object):\n",
    "    '''\n",
    "    Data是解析某一个xml文件的类\n",
    "    Input:\n",
    "        filename:string\n",
    "    Variable name:\n",
    "        name: string, eg:'16.jpg'\n",
    "        objs: list, the list of the Bbox\n",
    "    '''\n",
    "    def __init__(self,filename):\n",
    "        tree = ET.parse(filename)\n",
    "        root = tree.getroot()\n",
    "        self.name = root[1].text\n",
    "        self.objs = []\n",
    "        for obj in tree.findall('object'):\n",
    "            bbox= Bbox(obj[0].text,obj[4][0].text,obj[4][1].text,obj[4][2].text,obj[4][3].text)\n",
    "            self.objs.append(bbox)\n",
    "\n",
    "# data = Data('Dataset/TestLabel/15.xml')\n",
    "# print(data.name)\n",
    "# for obj in data.objs:\n",
    "#     print(obj.type,obj.xmin,obj.ymin,obj.xmax,obj.ymax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 建立pytorch数据集导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "class Datain(Dataset):\n",
    "    def __init__(self,ImagePath,LabelPath):\n",
    "        self.ImagePath = ImagePath\n",
    "        self.LabelPath = LabelPath\n",
    "        self.LabelList = os.listdir(self.LabelPath)\n",
    "        self.length = len(self.LabelList)\n",
    "        self.classes = {'robot':0, 'deck':1}\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        data = Data(os.path.join(self.LabelPath,self.LabelList[index]))\n",
    "        img = cv2.imread(os.path.join(self.ImagePath,data.name))\n",
    "        # 这里要将我们的格式转为yolo格式\n",
    "        yolo_formats = []\n",
    "        for obj in data.objs:\n",
    "            yolo_format = []\n",
    "            yolo_format.append(0)\n",
    "            yolo_format.append(self.classes[obj.type])\n",
    "            yolo_format.append((obj.xmin+obj.xmax)/2.0/416)\n",
    "            yolo_format.append((obj.ymin+obj.ymax)/2.0/416)\n",
    "            yolo_format.append(abs(obj.xmax-obj.xmin)/416.0)\n",
    "            yolo_format.append(abs(obj.ymax-obj.ymin)/416.0)\n",
    "            yolo_formats.append(yolo_format)\n",
    "        \n",
    "        return torch.Tensor(img).permute(2,0,1),torch.Tensor(yolo_formats)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "# datain = Datain('Dataset/TrainSet','Dataset/TrainLabel')\n",
    "# print(datain[0][0].shape,datain[0][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start\n",
      "load model sucessfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fair/anaconda3/envs/gpxtensor/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 EpochLoss:117.2436 EpochTestLoss:1775.3516\n",
      "epoch:1 EpochLoss:121.0086 EpochTestLoss:1705.0961\n",
      "epoch:2 EpochLoss:140.2013 EpochTestLoss:1642.3435\n",
      "epoch:3 EpochLoss:77.2266 EpochTestLoss:1677.5955\n",
      "epoch:4 EpochLoss:115.9254 EpochTestLoss:1621.9961\n",
      "epoch:5 EpochLoss:103.4783 EpochTestLoss:1681.0918\n",
      "epoch:6 EpochLoss:74.8958 EpochTestLoss:1741.7534\n",
      "epoch:7 EpochLoss:60.1551 EpochTestLoss:1727.8789\n",
      "epoch:8 EpochLoss:96.5804 EpochTestLoss:1701.2389\n",
      "epoch:9 EpochLoss:84.5902 EpochTestLoss:1723.4767\n",
      "epoch:10 EpochLoss:59.5639 EpochTestLoss:1755.6604\n",
      "epoch:11 EpochLoss:65.8744 EpochTestLoss:1738.4174\n",
      "epoch:12 EpochLoss:77.4171 EpochTestLoss:1741.9878\n",
      "epoch:13 EpochLoss:56.5895 EpochTestLoss:1816.3230\n",
      "epoch:14 EpochLoss:49.5267 EpochTestLoss:1802.5797\n",
      "epoch:15 EpochLoss:39.4507 EpochTestLoss:1887.2592\n",
      "epoch:16 EpochLoss:47.8740 EpochTestLoss:1863.5504\n",
      "epoch:17 EpochLoss:66.9711 EpochTestLoss:1889.7942\n",
      "epoch:18 EpochLoss:48.4994 EpochTestLoss:2017.9467\n",
      "epoch:19 EpochLoss:41.9742 EpochTestLoss:1891.2178\n",
      "epoch:20 EpochLoss:36.2590 EpochTestLoss:1916.1985\n",
      "epoch:21 EpochLoss:29.6480 EpochTestLoss:1940.8921\n",
      "epoch:22 EpochLoss:40.6697 EpochTestLoss:1936.2690\n",
      "epoch:23 EpochLoss:43.5225 EpochTestLoss:1963.5739\n",
      "epoch:24 EpochLoss:21.8599 EpochTestLoss:1991.3798\n",
      "epoch:25 EpochLoss:17.2083 EpochTestLoss:2007.9153\n",
      "epoch:26 EpochLoss:27.6178 EpochTestLoss:2008.6125\n",
      "epoch:27 EpochLoss:26.5402 EpochTestLoss:2028.8678\n",
      "epoch:28 EpochLoss:18.0715 EpochTestLoss:2073.6653\n",
      "epoch:29 EpochLoss:20.2860 EpochTestLoss:2071.1880\n",
      "epoch:30 EpochLoss:41.8461 EpochTestLoss:2069.3660\n",
      "epoch:31 EpochLoss:28.5626 EpochTestLoss:2023.1177\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# 初始化\n",
    "ImagePath = 'Dataset/TrainSet'\n",
    "LabelPath = 'Dataset/TrainLabel'\n",
    "TestImg='Dataset/TestSet'\n",
    "TestLabel='Dataset/TestLabel'\n",
    "lr = 0.0005\n",
    "BatchSize = 1\n",
    "Epochs = 100\n",
    "\n",
    "#指定第2块GPU进行训练\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "\n",
    "# 加载数据\n",
    "TrainData=Datain(ImagePath,LabelPath)\n",
    "TrainLoader = DataLoader(dataset=TrainData,batch_size=BatchSize,shuffle=True)\n",
    "\n",
    "TestData=Datain(TestImg,TestLabel)\n",
    "TestLoader=DataLoader(dataset=TestData,batch_size=BatchSize,shuffle=False)\n",
    "\n",
    "# 导入模型\n",
    "nano = YoloNano(class_num=2)\n",
    "print(\"Training Start\")\n",
    "nano.cuda()\n",
    "if os.path.exists('yolonano.pkl'):\n",
    "    nano.load_state_dict(torch.load('yolonano.pkl'))\n",
    "    print(\"load model sucessfully\")\n",
    "    \n",
    "optim = torch.optim.Adam(nano.parameters(),lr = lr,betas=(0.9,0.99))\n",
    "\n",
    "# 设置学习率下降策略\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim,step_size=1,gamma = 0.98)\n",
    "nano.train()\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    #更新学习率\n",
    "    scheduler.step()\n",
    "    EpochTrainLoss = 0\n",
    "    for batch,data in enumerate(TrainLoader):\n",
    "#         print(data[0].shape,data[1][0].shape)\n",
    "        Targets = data[1][0]\n",
    "        Input = data[0]\n",
    "        output1,output2,output3, loss = nano(Input.cuda(),Targets.cuda())\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        EpochTrainLoss += loss\n",
    "#         print('epoch:{} batch:{} loss:{}'.format(epoch, batch, loss))\n",
    "    EpochTestLoss=0\n",
    "    for batch,data in enumerate(TestLoader):\n",
    "        Targets = data[1][0]\n",
    "        Input = data[0]\n",
    "        output1,output2,output3, loss = nano(Input.cuda(),Targets.cuda())\n",
    "        EpochTestLoss+=loss\n",
    "    print('epoch:{} EpochLoss:{:.4f} EpochTestLoss:{:.4f}'.format(epoch, EpochTrainLoss,EpochTestLoss))\n",
    "    torch.save(nano.state_dict(), 'yolonano.pkl')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
